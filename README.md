# ğŸ¥ Lip Reading with Deep Learning  

A deep learning project that predicts spoken words from silent videos using **CNN + BiLSTM + CTC Loss**.  
Trained on the **GRID dataset** (20 speakers) and evaluated on unseen speakers, this project demonstrates end-to-end lip reading â€” from raw data preprocessing to model training, fine-tuning, evaluation, and deployment with a Streamlit UI.  

---

## ğŸ“‚ Project Overview  

- **Dataset**: GRID Corpus (~15GB)  
  - 20 Speakers (S1â€“S20) â†’ Training  
  - 5 Speakers (S21â€“S25) â†’ Unseen Evaluation  
- **Preprocessing**:  
  - Extracted frames from videos using OpenCV & ffmpeg  
  - Stored aligned transcripts in JSON format  
  - Normalized and resized frames to **64Ã—64**  
- **Model**:  
  - CNN for feature extraction  
  - BiLSTM for temporal modeling  
  - CTC Loss for alignment-free sequence prediction  

---

## âš™ï¸ Tech Stack  

- **Programming**: Python  
- **Deep Learning**: PyTorch  
- **Data Processing**: OpenCV, NumPy, Pandas  
- **Visualization**: Matplotlib, Seaborn  
- **Deployment**: Streamlit  
- **Others**: ffmpeg for frame extraction  

---

## ğŸ“Š Training  

- **Batch Size**: 4  
- **Optimizer**: Adam  
- **Learning Rate**: 1e-4  
- **Epochs**: 20  

Training Loss decreased from **1.08 â†’ 0.17** across 20 epochs.  

- CER (Character Error Rate) and WER (Word Error Rate) were tracked across epochs.  
- Loss curves and error metrics showed exponential improvement during training.  

---

## ğŸš€ Results  

### On Training Speakers (S1â€“S20)  
- CER â‰ˆ **0.05**  
- WER â‰ˆ **0.16**  

### On Unseen Speakers (S21â€“S25)  
- Before Fine-Tuning: CER â‰ˆ **0.21**, WER â‰ˆ **0.44**  
- After Fine-Tuning: CER â‰ˆ **0.05**, WER â‰ˆ **0.13**  

ğŸ“Œ Fine-tuning with transfer learning drastically reduced errors and improved generalization.  

---

## ğŸ“ˆ Visualizations  

- ğŸ“‰ Training Loss Curve  
- ğŸ“Š CER / WER Distributions  
- ğŸ”¥ Confusion Matrix (character-level)  
- ğŸ¯ Streamlit UI for video-level predictions  

*(Add your graphs/screenshots here)*  

---

## ğŸ–¥ï¸ Streamlit App  

An interactive Streamlit dashboard was built to:  

- Select speaker + video â†’ Get prediction  
- Compare **Predicted vs Actual transcript**  
- Display **CER/WER metrics**  
- Evaluate dataset-wide performance with graphs (histograms, averages, confusion matrix)
## ğŸ“Œ Key Learnings

Lip reading is highly challenging due to visually similar lip movements.

Fine-tuning bridged the gap between seen vs unseen speakers.

Visualization of CER/WER distributions and confusion matrices was crucial to understand model behavior.

Built a full ML pipeline: raw data â†’ preprocessing â†’ training â†’ fine-tuning â†’ evaluation â†’ deployment.

## ğŸ”® Future Work

Transformer-based architectures for lip reading

Multilingual lip reading (beyond English)

Audio-visual fusion (combine audio + video for robust recognition)
## ğŸ—ï¸ Project Structure
```
â”œâ”€â”€ data/                  # Raw GRID dataset (videos + audios)
â”œâ”€â”€ frames/                # Extracted frames (S1â€“S20)
â”œâ”€â”€ frames_unseen/         # Extracted frames (S21â€“S25)
â”œâ”€â”€ labels.json            # Transcripts for training speakers
â”œâ”€â”€ labels_new.json        # Transcripts for unseen speakers
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py          # Model training
|   â”œâ”€â”€ preprocess.py          # extract frames from video S1 to S20
|   â”œâ”€â”€ preprocess_new.py          # extract frames from video S21 to S25
|   â”œâ”€â”€ labels_extract.py    # transcript extract from S1 to S20 video
|   â”œâ”€â”€ labels_extract_new.py          # transcript extract from s21 to s25 video
â”‚   â”œâ”€â”€ finetune.py        # Fine-tuning unseen data
â”‚   â”œâ”€â”€ predict.py         # Predictions on training data
â”‚   â”œâ”€â”€ predict_new.py     # Predictions on unseen data
â”‚   â”œâ”€â”€ predict_finetuned.py # Predictions using fine-tuned model
|   â”œâ”€â”€ evaluate_results.py          # plot graph for model_performance
|   â”œâ”€â”€ evaluation_graphs.py          # evaluate graphs for CER, WER on data
|   â”œâ”€â”€ shape_predictor_68_face_landmarks.dat # used to locate the mouth region for lip reading preprocessing.
â”‚   â”œâ”€â”€ app.py             # Streamlit UI
â”œâ”€â”€ lipreading_model_final.pth          # Model saved after training
â”œâ”€â”€ requirements.txt          # project related dependencies
â”œâ”€â”€ lipreading_model_finetuned.pth          # Model saved after finetuning
â”œâ”€â”€ results.csv            # Predictions on training data
â”œâ”€â”€ results_new.csv        # Predictions on unseen speakers
â”œâ”€â”€ results_finetuned.csv  # Predictions after fine-tuning
â”œâ”€â”€ model_performance_summary.csv  # csv results of model in predictions
â””â”€â”€ README.md
```
Run locally:  
```bash
streamlit run src/app.py
```






